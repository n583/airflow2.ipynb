{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fd0ab2f-74f5-4f00-bbaf-b2b950c77759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipykernel_82669/1131260652.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">32</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: This module is deprecated. Please use `airflow.operators.bash`.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/tmp/ipykernel_82669/\u001b[0m\u001b[1;33m1131260652.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m32\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: This module is deprecated. Please use `airflow.operators.bash`.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipykernel_82669/1131260652.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">33</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: This module is deprecated. Please use `airflow.operators.python`.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/tmp/ipykernel_82669/\u001b[0m\u001b[1;33m1131260652.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m33\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: This module is deprecated. Please use `airflow.operators.python`.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'airflow.providers.apache'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mairflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbash_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BashOperator\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mairflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PythonOperator\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mairflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhive_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HiveOperator\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m date, timedelta\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Create a few placeholder scripts. In practice these would be different python\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# script files, which are imported in this section with absolute or relative imports\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/airflow/operators/hive_operator.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"This module is deprecated. Please use :mod:`airflow.providers.apache.hive.operators.hive`.\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mairflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproviders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapache\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhive\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhive\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HiveOperator  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     24\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis module is deprecated. Please use `airflow.providers.apache.hive.operators.hive`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     28\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'airflow.providers.apache'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "# --------------------------------------------------------------------------------\n",
    "# Written By: Ekhtiar Syed\n",
    "# Last Update: 8th April 2016\n",
    "# Caveat: This Dag will not run because of missing scripts.\n",
    "# The purpose of this is to give you a sample of a real world example DAG!\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Load The Dependencies\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.hive_operator import HiveOperator\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create a few placeholder scripts. In practice these would be different python\n",
    "# script files, which are imported in this section with absolute or relative imports\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def fetchtweets():\n",
    "    return None\n",
    "\n",
    "\n",
    "def cleantweets():\n",
    "    return None\n",
    "\n",
    "\n",
    "def analyzetweets():\n",
    "    return None\n",
    "\n",
    "\n",
    "def transfertodb():\n",
    "    return None\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# set default arguments\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': airflow.utils.dates.days_ago(2),\n",
    "    'email': ['airflow@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    # 'queue': 'bash_queue',\n",
    "    # 'pool': 'backfill',\n",
    "    # 'priority_weight': 10,\n",
    "    # 'end_date': datetime(2016, 1, 1),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'example_twitter_dag', default_args=default_args,\n",
    "    schedule_interval=\"@daily\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# This task should call Twitter API and retrieve tweets from yesterday from and to\n",
    "# for the four twitter users (Twitter_A,..,Twitter_D) There should be eight csv\n",
    "# output files generated by this task and naming convention\n",
    "# is direction(from or to)_twitterHandle_date.csv\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "fetch_tweets = PythonOperator(\n",
    "    task_id='fetch_tweets',\n",
    "    python_callable=fetchtweets,\n",
    "    dag=dag)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Clean the eight files. In this step you can get rid of or cherry pick columns\n",
    "# and different parts of the text\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "clean_tweets = PythonOperator(\n",
    "    task_id='clean_tweets',\n",
    "    python_callable=cleantweets,\n",
    "    dag=dag)\n",
    "\n",
    "clean_tweets.set_upstream(fetch_tweets)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# In this section you can use a script to analyze the twitter data. Could simply\n",
    "# be a sentiment analysis through algorithms like bag of words or something more\n",
    "# complicated. You can also take a look at Web Services to do such tasks\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "analyze_tweets = PythonOperator(\n",
    "    task_id='analyze_tweets',\n",
    "    python_callable=analyzetweets,\n",
    "    dag=dag)\n",
    "\n",
    "analyze_tweets.set_upstream(clean_tweets)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Although this is the last task, we need to declare it before the next tasks as we\n",
    "# will use set_downstream This task will extract summary from Hive data and store\n",
    "# it to MySQL\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "hive_to_mysql = PythonOperator(\n",
    "    task_id='hive_to_mysql',\n",
    "    python_callable=transfertodb,\n",
    "    dag=dag)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# The following tasks are generated using for loop. The first task puts the eight\n",
    "# csv files to HDFS. The second task loads these files from HDFS to respected Hive\n",
    "# tables. These two for loops could be combined into one loop. However, in most cases,\n",
    "# you will be running different analysis on your incoming incoming and outgoing tweets,\n",
    "# and hence they are kept separated in this example.\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "from_channels = ['fromTwitter_A', 'fromTwitter_B', 'fromTwitter_C', 'fromTwitter_D']\n",
    "to_channels = ['toTwitter_A', 'toTwitter_B', 'toTwitter_C', 'toTwitter_D']\n",
    "yesterday = date.today() - timedelta(days=1)\n",
    "dt = yesterday.strftime(\"%Y-%m-%d\")\n",
    "# define where you want to store the tweets csv file in your local directory\n",
    "local_dir = \"/tmp/\"\n",
    "# define the location where you want to store in HDFS\n",
    "hdfs_dir = \" /tmp/\"\n",
    "\n",
    "for channel in to_channels:\n",
    "\n",
    "    file_name = \"to_\" + channel + \"_\" + yesterday.strftime(\"%Y-%m-%d\") + \".csv\"\n",
    "\n",
    "    load_to_hdfs = BashOperator(\n",
    "        task_id=\"put_\" + channel + \"_to_hdfs\",\n",
    "        bash_command=\"HADOOP_USER_NAME=hdfs hadoop fs -put -f \" +\n",
    "                     local_dir + file_name +\n",
    "                     hdfs_dir + channel + \"/\",\n",
    "        dag=dag)\n",
    "\n",
    "    load_to_hdfs.0.0.0.0:8080\n",
    "(analyze_tweets)\n",
    "\n",
    "    load_to_hive = HiveOperator(\n",
    "        task_id=\"load_\" + channel + \"_to_hive\",\n",
    "        hql=\"LOAD DATA INPATH '\" +\n",
    "            hdfs_dir + channel + \"/\" + file_name + \"' \"\n",
    "            \"INTO TABLE \" + channel + \" \"\n",
    "            \"PARTITION(dt='\" + dt + \"')\",\n",
    "        dag=dag)\n",
    "    load_to_hive.set_upstream(load_to_hdfs)\n",
    "    load_to_hive.set_downstream(hive_to_mysql)\n",
    "\n",
    "for channel in from_channels:\n",
    "    file_name = \"from_\" + channel + \"_\" + yesterday.strftime(\"%Y-%m-%d\") + \".csv\"\n",
    "    load_to_hdfs = BashOperator(\n",
    "        task_id=\"put_\" + channel + \"_to_hdfs\",\n",
    "        bash_command=\"HADOOP_USER_NAME=hdfs hadoop fs -put -f \" +\n",
    "                     local_dir + file_name +\n",
    "                     hdfs_dir + channel + \"/\",\n",
    "        dag=dag)\n",
    "\n",
    "    load_to_hdfs.set_upstream(analyze_tweets)\n",
    "\n",
    "    load_to_hive = HiveOperator(\n",
    "        task_id=\"load_\" + channel + \"_to_hive\",\n",
    "        hql=\"LOAD DATA INPATH '\" +\n",
    "            hdfs_dir + channel + \"/\" + file_name + \"' \"\n",
    "            \"INTO TABLE \" + channel + \" \"\n",
    "            \"PARTITION(dt='\" + dt + \"')\",\n",
    "        dag=dag)\n",
    "\n",
    "    load_to_hive.set_upstream(load_to_hdfs)\n",
    "    load_to_hive.set_downstream(hive_to_mysql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d4c3e-04be-4f03-9654-d99099b9fcbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
